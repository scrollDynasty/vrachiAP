#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –ø–æ —ç–ø–æ—Ö–∞–º
"""

import os
import sys
import json
import torch
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import logging
import time
from typing import Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neural_network import MedicalDiagnosisNetwork, MedicalDataProcessor, MedicalAITrainer, MegaMedicalDiagnosisNetwork
from data_downloader import MassiveMedicalDataGenerator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MedicalModelTrainer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
    
    def __init__(self, data_dir: str = "medical_data", models_dir: str = "trained_models"):
        self.data_dir = data_dir
        self.models_dir = models_dir
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(models_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.data_processor = MedicalDataProcessor()
        self.downloader = MassiveMedicalDataGenerator(data_dir)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏ (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ú–û–©–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê)
        self.hidden_sizes = [4096, 2048, 1024, 512, 256, 128, 64]  # –û–ì–†–û–ú–ù–ê–Ø –≥–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å
        self.dropout_rate = 0.3  # –£–º–µ—Ä–µ–Ω–Ω—ã–π dropout –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        self.batch_size = 32  # –ú–µ–Ω—å—à–µ –±–∞—Ç—á –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        self.learning_rate = 0.005  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        self.embedding_dim = 512  # –ë–æ–ª—å—à–∏–µ embeddings
        self.num_attention_heads = 16  # –ú–Ω–æ–≥–æ attention heads
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
    
    def download_data(self) -> bool:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîÑ –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            dataset = self.downloader.generate_massive_dataset(100000)  # 100k –∑–∞–ø–∏—Å–µ–π
            dataset = self.downloader.add_noise_and_variations(dataset)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            result = self.downloader.save_massive_dataset(dataset, "training_data")
            
            if result and result['total_records'] > 0:
                logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {result['total_records']} –∑–∞–ø–∏—Å–µ–π")
                return True
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
                return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def generate_incremental_data(self, records_count: int = 50000) -> bool:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)"""
        logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {records_count:,} –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö
            existing_file = None
            data_files = []
            
            for filename in os.listdir(self.data_dir):
                if filename.startswith('medical_') and filename.endswith('.json') and not 'stats' in filename:
                    filepath = os.path.join(self.data_dir, filename)
                    size = os.path.getsize(filepath)
                    data_files.append((filepath, size, filename))
            
            if data_files:
                # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª –∫–∞–∫ –æ—Å–Ω–æ–≤—É
                data_files.sort(key=lambda x: x[1], reverse=True)
                existing_file = data_files[0][0]
                logger.info(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {data_files[0][2]}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            new_data_file = self.downloader.generate_incremental_dataset(
                records_count=records_count,
                existing_data_file=existing_file or ""
            )
            
            if new_data_file and os.path.exists(new_data_file):
                logger.info(f"‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã: {new_data_file}")
                return True
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def prepare_data(self) -> Optional[tuple]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üîÑ –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            # –ò—â–µ–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö
            data_files = []
            for filename in os.listdir(self.data_dir):
                if filename.startswith('medical_') and filename.endswith('.json') and not 'stats' in filename:
                    filepath = os.path.join(self.data_dir, filename)
                    size = os.path.getsize(filepath)
                    data_files.append((filepath, size, filename))
            
            if not data_files:
                logger.error(f"‚ùå –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤: {self.data_dir}")
                return None
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞ (—Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –ø–µ—Ä–≤—ã–π)
            data_files.sort(key=lambda x: x[1], reverse=True)
            data_file = data_files[0][0]
            logger.info(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {data_files[0][2]} ({data_files[0][1]//1024//1024} –ú–ë)")
            
            data, symptoms, diseases = self.data_processor.load_data(data_file)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
            self.data_processor.create_vocabulary(symptoms, diseases)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            if len(data) > 1000000:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 1 –º–∏–ª–ª–∏–æ–Ω–∞ –∑–∞–ø–∏—Å–µ–π
                logger.info(f"üéØ –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏: {len(data):,} -> 1,000,000 –∑–∞–ø–∏—Å–µ–π")
                data = data[:1000000]
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            X, y = self.data_processor.prepare_training_data(data)
            
            if len(X) == 0:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return None
            
            # –ê—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ (–æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
            logger.info("üíæ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")
            X_augmented, y_augmented = X, y  # –ë–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
            test_size = min(0.2, 0.1 * len(X_augmented) / len(set(y_augmented)))
            test_size = max(test_size, 0.05)  # –ú–∏–Ω–∏–º—É–º 5%
            
            X_train, X_val, y_train, y_val = train_test_split(
                X_augmented, y_augmented, test_size=test_size, random_state=42, 
                stratify=y_augmented if len(set(y_augmented)) <= len(y_augmented) * 0.5 else None
            )
            
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
            logger.info(f"   - –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            logger.info(f"   - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            logger.info(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–ø—Ç–æ–º–æ–≤: {len(symptoms)}")
            logger.info(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π: {len(diseases)}")
            
            return X_train, X_val, y_train, y_val, len(symptoms), len(diseases)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def create_model(self, input_size: int, output_size: int) -> MegaMedicalDiagnosisNetwork:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ú–ï–ì–ê –ú–û–©–ù–û–ô –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        logger.info("üîÑ –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ú–û–©–ù–û–ô –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
        
        model = MegaMedicalDiagnosisNetwork(
            input_size=input_size,
            output_size=output_size,
            hidden_sizes=self.hidden_sizes,
            dropout_rate=self.dropout_rate,
            embedding_dim=self.embedding_dim,
            num_attention_heads=self.num_attention_heads
        )
        
        logger.info(f"‚úÖ –ú–ï–ì–ê-–ú–û–î–ï–õ–¨ —Å–æ–∑–¥–∞–Ω–∞:")
        logger.info(f"   - –í—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {input_size}")
        logger.info(f"   - –í—ã—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {output_size}")
        logger.info(f"   - Transformer —Å–ª–æ–µ–≤: 6")
        logger.info(f"   - Attention heads: {self.num_attention_heads}")
        logger.info(f"   - Embedding: {self.embedding_dim}")
        
        return model
    
    def train_model(self, model, X_train, X_val, y_train, y_val, num_epochs: int = 100, patience: int = 50) -> float:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        logger.info(f"üîÑ –®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞ {num_epochs} —ç–ø–æ—Ö...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train),
                torch.LongTensor(y_train)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val),
                torch.LongTensor(y_val)
            )
            
            train_loader = DataLoader(
                train_dataset, 
                batch_size=self.batch_size, 
                shuffle=True,
                drop_last=True
            )
            val_loader = DataLoader(
                val_dataset, 
                batch_size=self.batch_size, 
                shuffle=False
            )
            
            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫
            trainer = MedicalAITrainer(model)
            
            # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            save_path = os.path.join(self.models_dir, 'best_medical_model.pth')
            
            # –û–±—É—á–µ–Ω–∏–µ
            start_time = time.time()
            best_accuracy = trainer.train(
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=num_epochs,
                save_path=save_path,
                patience=patience
            )
            end_time = time.time()
            
            training_time = end_time - start_time
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"   - –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
            self.save_training_info(trainer, save_path)
            
            return best_accuracy
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            return 0.0
    
    def save_training_info(self, trainer: MedicalAITrainer, model_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö
            processor_path = os.path.join(self.models_dir, 'data_processor.json')
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å–∫–µ–π–ª–µ—Ä–∞
            try:
                scaler_mean = self.data_processor.scaler.mean_.tolist() if hasattr(self.data_processor.scaler.mean_, 'tolist') else []
                scaler_scale = self.data_processor.scaler.scale_.tolist() if hasattr(self.data_processor.scaler.scale_, 'tolist') else []
            except:
                scaler_mean = []
                scaler_scale = []
            
            processor_data = {
                'symptom_vocab': self.data_processor.symptom_vocab,
                'disease_vocab': self.data_processor.disease_vocab,
                'vocab_size': self.data_processor.vocab_size,
                'scaler_mean': scaler_mean,
                'scaler_scale': scaler_scale
            }
            
            with open(processor_path, 'w', encoding='utf-8') as f:
                json.dump(processor_data, f, ensure_ascii=False, indent=2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
            history_path = os.path.join(self.models_dir, 'training_history.json')
            history_data = {
                'train_losses': trainer.train_losses,
                'val_losses': trainer.val_losses,
                'train_accuracies': trainer.train_accuracies,
                'val_accuracies': trainer.val_accuracies,
                'model_architecture': {
                    'input_size': trainer.model.input_size,
                    'hidden_sizes': trainer.model.hidden_sizes,
                    'output_size': trainer.model.output_size,
                    'dropout_rate': trainer.model.dropout_rate
                }
            }
            
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
            logger.info(f"   - –ú–æ–¥–µ–ª—å: {model_path}")
            logger.info(f"   - –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö: {processor_path}")
            logger.info(f"   - –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è: {history_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
    
    def full_training_pipeline(self, num_epochs: int = 100, download_new_data: bool = True, patience: int = 1000) -> bool:
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
        logger.info("=" * 60)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–∫–∞—á–∏–≤–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        data_file = os.path.join(self.data_dir, 'medical_data.json')
        if download_new_data or not os.path.exists(data_file):
            if not self.download_data():
                return False
        else:
            logger.info("üìÇ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data_result = self.prepare_data()
        if data_result is None:
            return False
        
        X_train, X_val, y_train, y_val, input_size, output_size = data_result
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = self.create_model(input_size, output_size)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        best_accuracy = self.train_model(model, X_train, X_val, y_train, y_val, num_epochs, patience)
        
        if best_accuracy > 0:
            logger.info("=" * 60)
            logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
            logger.info(f"   - –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%")
            logger.info(f"   - –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.models_dir}")
            logger.info("=" * 60)
            return True
        else:
            logger.error("‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")
            return False
    
    def setup_transfer_learning(self, model_path: str, freeze_ratio: float = 0.5) -> bool:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ transfer learning –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ transfer learning –∏–∑ –º–æ–¥–µ–ª–∏: {model_path}")
            
            if not os.path.exists(model_path):
                logger.warning(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
                return False
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            data_file = os.path.join(self.data_dir, 'medical_data.json')
            if not os.path.exists(data_file):
                logger.warning("–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º transfer learning")
                return False
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data_result = self.prepare_data()
            if data_result is None:
                return False
            
            X_train, X_val, y_train, y_val, input_size, output_size = data_result
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            model = self.create_model(input_size, output_size)
            
            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
            trainer = MedicalAITrainer(model)
            
            if trainer.load_for_transfer_learning(model_path):
                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º transfer learning
                trainer.setup_transfer_learning(freeze_ratio)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                self.trainer = trainer
                self.model = model
                
                logger.info("‚úÖ Transfer learning –Ω–∞—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è transfer learning")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ transfer learning: {e}")
            return False
    
    def test_model(self, test_symptoms: list) -> dict:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model_path = os.path.join(self.models_dir, 'best_medical_model.pth')
            processor_path = os.path.join(self.models_dir, 'data_processor.json')
            
            if not os.path.exists(model_path) or not os.path.exists(processor_path):
                return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å."}
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö
            with open(processor_path, 'r', encoding='utf-8') as f:
                processor_data = json.load(f)
            
            self.data_processor.symptom_vocab = processor_data['symptom_vocab']
            self.data_processor.disease_vocab = processor_data['disease_vocab']
            self.data_processor.vocab_size = processor_data['vocab_size']
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–µ–π–ª–µ—Ä
            self.data_processor.scaler.mean_ = np.array(processor_data['scaler_mean'])
            self.data_processor.scaler.scale_ = np.array(processor_data['scaler_scale'])
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            checkpoint = torch.load(model_path, map_location='cpu')
            model = MedicalDiagnosisNetwork(
                input_size=checkpoint['input_size'],
                hidden_sizes=checkpoint['hidden_sizes'],
                output_size=checkpoint['output_size']
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            # –ö–æ–¥–∏—Ä—É–µ–º —Å–∏–º–ø—Ç–æ–º—ã
            symptom_vector = self.data_processor.encode_symptoms(test_symptoms)
            symptom_vector = self.data_processor.scaler.transform([symptom_vector])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                input_tensor = torch.FloatTensor(symptom_vector)
                output, confidence = model(input_tensor)
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                probabilities = torch.softmax(output, dim=1)
                top_probs, top_indices = torch.topk(probabilities, k=min(3, output.size(1)))
                
                results = []
                for i in range(top_probs.size(1)):
                    disease_idx = top_indices[0][i].item()
                    probability = top_probs[0][i].item()
                    disease_name = self.data_processor.decode_disease(int(disease_idx))
                    
                    results.append({
                        'disease': disease_name,
                        'probability': probability * 100,
                        'confidence': confidence[0][0].item() * 100
                    })
                
                return {
                    'predictions': results,
                    'input_symptoms': test_symptoms
                }
        
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {str(e)}"}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏')
    parser.add_argument('--epochs', type=int, default=100, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--download', action='store_true', help='–°–∫–∞—á–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ')
    parser.add_argument('--incremental', action='store_true', help='–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--test', nargs='*', help='–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å —Å–∏–º–ø—Ç–æ–º–∞–º–∏')
    
    args = parser.parse_args()
    
    trainer = MedicalModelTrainer()
    
    if args.test is not None:
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        if len(args.test) == 0:
            # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–∏–º–ø—Ç–æ–º—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            test_symptoms = ['–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '–∫–∞—à–µ–ª—å', '–Ω–∞—Å–º–æ—Ä–∫']
        else:
            test_symptoms = args.test
        
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Å–∏–º–ø—Ç–æ–º–∞–º–∏: {test_symptoms}")
        result = trainer.test_model(test_symptoms)
        
        if 'error' in result:
            print(f"‚ùå {result['error']}")
        else:
            print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏:")
            for pred in result['predictions']:
                print(f"   - {pred['disease']}: {pred['probability']:.1f}% (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {pred['confidence']:.1f}%)")
    else:
        # –û–±—É—á–µ–Ω–∏–µ
        if args.incremental:
            print("üîÑ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if trainer.generate_incremental_data(records_count=50000):
                # –û–±—É—á–∞–µ–º —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —ç–ø–æ—Ö
                success = trainer.full_training_pipeline(
                    num_epochs=min(args.epochs, 50),
                    download_new_data=False
                )
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                success = False
        else:
            # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            success = trainer.full_training_pipeline(
                num_epochs=args.epochs,
                download_new_data=args.download
            )
        
        if success:
            print("\nüéØ –ú–æ–∂–µ—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π:")
            print("python train_model.py --test –≥–æ–ª–æ–≤–Ω–∞—è_–±–æ–ª—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∫–∞—à–µ–ª—å")
        else:
            print("‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")

if __name__ == "__main__":
    main() 