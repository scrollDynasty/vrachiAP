#!/usr/bin/env python3
"""
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Transfer Learning
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import os
from typing import Dict, List, Tuple, Optional
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import logging
from datetime import datetime
import shutil

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MedicalDiagnosisNetwork(nn.Module):
    """–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Transfer Learning"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, dropout_rate: float = 0.3):
        super(MedicalDiagnosisNetwork, self).__init__()
        
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_sizes = hidden_sizes
        self.dropout_rate = dropout_rate
        
        layers = []
        
        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(input_size, hidden_sizes[0]))
        layers.append(nn.BatchNorm1d(hidden_sizes[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))
        
        # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ —Å BatchNorm
        for i in range(len(hidden_sizes) - 1):
            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))
            layers.append(nn.BatchNorm1d(hidden_sizes[i + 1]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–±–µ–∑ BatchNorm)
        layers.append(nn.Linear(hidden_sizes[-1], output_size))
        
        self.network = nn.Sequential(*layers)
        
        # –°–ª–æ–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        self.confidence_layer = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ"""
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—É—é —Å–µ—Ç—å
        hidden = x
        for layer in self.network[:-1]:
            hidden = layer(hidden)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        last_hidden = hidden
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        output = self.network[-1](hidden)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = self.confidence_layer(last_hidden)
        
        return output, confidence
    
    def freeze_layers(self, freeze_ratio: float = 0.5):
        """–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —á–∞—Å—Ç—å —Å–ª–æ–µ–≤ –¥–ª—è transfer learning"""
        layers_to_freeze = int(len(self.network) * freeze_ratio)
        
        for i, layer in enumerate(self.network):
            if i < layers_to_freeze:
                for param in layer.parameters():
                    param.requires_grad = False
                logger.info(f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω —Å–ª–æ–π {i}: {layer}")
            else:
                for param in layer.parameters():
                    param.requires_grad = True
        
        logger.info(f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω–æ {layers_to_freeze} –∏–∑ {len(self.network)} —Å–ª–æ–µ–≤")
    
    def unfreeze_all_layers(self):
        """–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Å–ª–æ–∏"""
        for layer in self.network:
            for param in layer.parameters():
                param.requires_grad = True
        logger.info("–í—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã")

class MedicalDataProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.symptom_vocab = {}
        self.disease_vocab = {}
        self.vocab_size = 0
        self.scaler = StandardScaler()
        
    def load_data(self, data_file: str) -> Tuple[List[Dict], List[str], List[str]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(data_file):
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∏–º–ø—Ç–æ–º—ã –∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
        all_symptoms = set()
        all_diseases = set()
        
        for item in data:
            if 'symptoms' in item and item['symptoms']:
                all_symptoms.update(item['symptoms'])
            if 'name' in item:
                all_diseases.add(item['name'])
        
        return data, list(all_symptoms), list(all_diseases)
    
    def create_vocabulary(self, symptoms: List[str], diseases: List[str]):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è —Å–∏–º–ø—Ç–æ–º–æ–≤ –∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π"""
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–ø—Ç–æ–º–æ–≤
        self.symptom_vocab = {symptom: i for i, symptom in enumerate(symptoms)}
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π
        self.disease_vocab = {disease: i for i, disease in enumerate(diseases)}
        
        self.vocab_size = len(symptoms)
        
        logger.info(f"–°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å: {len(symptoms)} —Å–∏–º–ø—Ç–æ–º–æ–≤, {len(diseases)} –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π")
    
    def extend_vocabulary(self, new_symptoms: List[str], new_diseases: List[str]):
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π –Ω–æ–≤—ã–º–∏ —Å–∏–º–ø—Ç–æ–º–∞–º–∏ –∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è–º–∏"""
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–∏–º–ø—Ç–æ–º—ã
        for symptom in new_symptoms:
            if symptom not in self.symptom_vocab:
                self.symptom_vocab[symptom] = len(self.symptom_vocab)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
        for disease in new_diseases:
            if disease not in self.disease_vocab:
                self.disease_vocab[disease] = len(self.disease_vocab)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        self.vocab_size = len(self.symptom_vocab)
        
        logger.info(f"–°–ª–æ–≤–∞—Ä—å —Ä–∞—Å—à–∏—Ä–µ–Ω: {len(self.symptom_vocab)} —Å–∏–º–ø—Ç–æ–º–æ–≤, {len(self.disease_vocab)} –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π")
    
    def encode_symptoms(self, symptoms: List[str]) -> np.ndarray:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–º–ø—Ç–æ–º–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä"""
        vector = np.zeros(self.vocab_size)
        
        for symptom in symptoms:
            # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if symptom in self.symptom_vocab:
                vector[self.symptom_vocab[symptom]] = 1.0
            else:
                # –ò—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                for vocab_symptom, idx in self.symptom_vocab.items():
                    if symptom.lower() in vocab_symptom.lower() or vocab_symptom.lower() in symptom.lower():
                        vector[idx] = 0.5  # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                        break
        
        return vector
    
    def encode_disease(self, disease: str) -> int:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è"""
        if disease in self.disease_vocab:
            return self.disease_vocab[disease]
        return -1
    
    def decode_disease(self, disease_idx: int) -> str:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è"""
        for disease, idx in self.disease_vocab.items():
            if idx == disease_idx:
                return disease
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"
    
    def prepare_training_data(self, data: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        X = []
        y = []
        
        for item in data:
            if 'symptoms' in item and item['symptoms'] and 'name' in item:
                # –ö–æ–¥–∏—Ä—É–µ–º —Å–∏–º–ø—Ç–æ–º—ã
                symptom_vector = self.encode_symptoms(item['symptoms'])
                
                # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ
                disease_idx = self.encode_disease(item['name'])
                
                if disease_idx != -1:
                    X.append(symptom_vector)
                    y.append(disease_idx)
        
        X = np.array(X)
        y = np.array(y)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        X = self.scaler.fit_transform(X)
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return X, y
    
    def add_noise(self, X: np.ndarray, noise_level: float = 0.1) -> np.ndarray:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        noise = np.random.normal(0, noise_level, X.shape)
        return X + noise
    
    def augment_data(self, X: np.ndarray, y: np.ndarray, augmentation_factor: int = 1) -> Tuple[np.ndarray, np.ndarray]:
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º {augmentation_factor}")
        
        X_augmented = [X]
        y_augmented = [y]
        
        for _ in range(augmentation_factor - 1):
            X_noisy = self.add_noise(X, noise_level=0.1)
            X_augmented.append(X_noisy)
            y_augmented.append(y)
        
        X_final = np.concatenate(X_augmented, axis=0)
        y_final = np.concatenate(y_augmented, axis=0)
        
        logger.info(f"–î–∞–Ω–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã: {len(X)} -> {len(X_final)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        return X_final, y_final

class MedicalAITrainer:
    """–¢—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Transfer Learning"""
    
    def __init__(self, model, device: Optional[str] = None):
        self.model = model
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (—É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π learning rate)
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)
        self.criterion = nn.CrossEntropyLoss()
        self.confidence_criterion = nn.MSELoss()
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π scheduler –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        
        # –î–ª—è transfer learning
        self.previous_best_accuracy = 0.0
        self.learning_rate_schedule = []
        
        logger.info(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
    
    def setup_transfer_learning(self, freeze_ratio: float = 0.5, fine_tuning_lr: float = 0.0001):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ transfer learning"""
        logger.info("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Transfer Learning...")
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —á–∞—Å—Ç—å —Å–ª–æ–µ–≤
        self.model.freeze_layers(freeze_ratio)
        
        # –£–º–µ–Ω—å—à–∞–µ–º learning rate –¥–ª—è fine-tuning
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = fine_tuning_lr
        
        logger.info(f"Transfer Learning –Ω–∞—Å—Ç—Ä–æ–µ–Ω: freeze_ratio={freeze_ratio}, lr={fine_tuning_lr}")
    
    def train_epoch(self, train_loader, epoch: int) -> Tuple[float, float]:
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            self.optimizer.zero_grad()
            
            # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            output, confidence = self.model(data)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            classification_loss = self.criterion(output, target)
            
            # –¶–µ–ª–µ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–≤—ã—Å–æ–∫–∞—è –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)
            predicted = torch.argmax(output, dim=1)
            target_confidence = (predicted == target).float().unsqueeze(1)
            confidence_loss = self.confidence_criterion(confidence, target_confidence)
            
            # –û–±—â–∞—è –ø–æ—Ç–µ—Ä—è
            total_loss_batch = classification_loss + 0.3 * confidence_loss
            
            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            total_loss_batch.backward()
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += total_loss_batch.item()
            correct += (predicted == target).sum().item()
            total += target.size(0)
            
            if batch_idx % 10 == 0:
                logger.info(f'–≠–ø–æ—Ö–∞ {epoch}, –ë–∞—Ç—á {batch_idx}, –ü–æ—Ç–µ—Ä—è: {total_loss_batch.item():.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {100. * correct / total:.2f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate_epoch(self, val_loader, epoch: int) -> Tuple[float, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output, confidence = self.model(data)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
                classification_loss = self.criterion(output, target)
                predicted = torch.argmax(output, dim=1)
                target_confidence = (predicted == target).float().unsqueeze(1)
                confidence_loss = self.confidence_criterion(confidence, target_confidence)
                
                total_loss_batch = classification_loss + 0.3 * confidence_loss
                total_loss += total_loss_batch.item()
                
                correct += (predicted == target).sum().item()
                total += target.size(0)
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader, num_epochs: int = 100, save_path: Optional[str] = None, 
              patience: int = 50, is_transfer_learning: bool = False) -> float:
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_epochs} —ç–ø–æ—Ö...")
        
        best_val_accuracy = self.previous_best_accuracy if is_transfer_learning else 0.0
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_accuracy = self.train_epoch(train_loader, epoch + 1)
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_accuracy)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_accuracy = self.validate_epoch(val_loader, epoch + 1)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_accuracy)
            
            logger.info(f'–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}:')
            logger.info(f'  –û–±—É—á–µ–Ω–∏–µ - –ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {train_accuracy:.2f}%')
            logger.info(f'  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ü–æ—Ç–µ—Ä—è: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.2f}%')
            
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏
            self.scheduler.step(val_loss)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            if save_path is not None and (epoch + 1) % 10 == 0:
                checkpoint_path = save_path.replace('.pth', f'_checkpoint_epoch_{epoch + 1}.pth')
                self.save_model(checkpoint_path, epoch + 1, val_accuracy)
                logger.info(f'üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: epoch {epoch + 1}')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                patience_counter = 0
                
                if save_path is not None:
                    self.save_model(save_path, epoch + 1, val_accuracy)
                    logger.info(f'‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {val_accuracy:.2f}%')
            else:
                patience_counter += 1
            
            # Early stopping
            if patience_counter >= patience:
                logger.info(f'Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}')
                break
        
        logger.info(f'–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_accuracy:.2f}%')
        return best_val_accuracy
    
    def save_model(self, save_path: str, epoch: int, accuracy: float):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'accuracy': accuracy,
            'input_size': self.model.input_size,
            'hidden_sizes': self.model.hidden_sizes,
            'output_size': self.model.output_size,
            'dropout_rate': self.model.dropout_rate,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'timestamp': datetime.now().isoformat()
        }, save_path)
    
    def load_for_transfer_learning(self, model_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è transfer learning"""
        try:
            if not os.path.exists(model_path):
                logger.warning(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
                return False
            
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∏—Å—Ç–æ—Ä–∏—é
            self.train_losses = checkpoint.get('train_losses', [])
            self.val_losses = checkpoint.get('val_losses', [])
            self.train_accuracies = checkpoint.get('train_accuracies', [])
            self.val_accuracies = checkpoint.get('val_accuracies', [])
            
            self.previous_best_accuracy = checkpoint.get('accuracy', 0.0)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è transfer learning")
            logger.info(f"   - –ü—Ä–µ–¥—ã–¥—É—â–∞—è –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {self.previous_best_accuracy:.2f}%")
            logger.info(f"   - –≠–ø–æ—Ö–∞: {checkpoint.get('epoch', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def create_model_backup(self, model_path: str, backup_dir: str = "model_backups"):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏"""
        if not os.path.exists(model_path):
            return
        
        os.makedirs(backup_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = os.path.join(backup_dir, f"model_backup_{timestamp}.pth")
        
        shutil.copy2(model_path, backup_path)
        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –º–æ–¥–µ–ª–∏: {backup_path}")

class MegaMedicalDiagnosisNetwork(nn.Module):
    """–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ú–û–©–ù–ê–Ø –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å –≥–ª—É–±–æ–∫–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏ attention"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, 
                 dropout_rate: float = 0.3, embedding_dim: int = 512, num_attention_heads: int = 16):
        super().__init__()
        
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_sizes = hidden_sizes
        self.dropout_rate = dropout_rate
        self.embedding_dim = embedding_dim
        
        # Input projection –∫ embedding space
        self.input_projection = nn.Linear(input_size, embedding_dim)
        
        # Multi-head attention –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–∏–º–ø—Ç–æ–º–∞–º–∏
        self.attention = nn.MultiheadAttention(
            embed_dim=embedding_dim, 
            num_heads=num_attention_heads, 
            dropout=dropout_rate,
            batch_first=True
        )
        
        # –û–ì–†–û–ú–ù–ê–Ø –≥–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å
        layers = []
        
        # –û—Ç embedding –∫ –ø–µ—Ä–≤–æ–º—É hidden —Å–ª–æ—é
        layers.extend([
            nn.Linear(embedding_dim, hidden_sizes[0]),
            nn.LayerNorm(hidden_sizes[0]),
            nn.GELU(),  # GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            nn.Dropout(dropout_rate)
        ])
        
        # –û–≥—Ä–æ–º–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ —Å BatchNorm
        for i in range(len(hidden_sizes) - 1):
            layers.extend([
                nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]),
                nn.LayerNorm(hidden_sizes[i + 1]),
                nn.GELU(),
                nn.Dropout(dropout_rate)
            ])
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(hidden_sizes[-1], output_size))
        
        self.network = nn.Sequential(*layers)
        
        # –ú–æ—â–Ω–∞—è confidence —Å–µ—Ç—å
        self.confidence_layer = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
        # Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
    
    def forward(self, x):
        batch_size = x.size(0)
        
        # Project –∫ embedding –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É
        x = self.input_projection(x)  # [batch, embedding_dim]
        
        # –î–æ–±–∞–≤–ª—è–µ–º sequence dimension –¥–ª—è attention
        x = x.unsqueeze(1)  # [batch, 1, embedding_dim]
        
        # Self-attention –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–≤—è–∑–µ–π
        attended, attention_weights = self.attention(x, x, x)
        x = attended.squeeze(1)  # [batch, embedding_dim]
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –º–µ–≥–∞-–≥–ª—É–±–æ–∫—É—é —Å–µ—Ç—å
        hidden = x
        for layer in self.network[:-1]:
            hidden = layer(hidden)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        output = self.network[-1](hidden)
        
        # Confidence estimation
        confidence = self.confidence_layer(hidden)
        
        return output, confidence
    
    def freeze_layers(self, freeze_ratio: float = 0.5):
        """–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —á–∞—Å—Ç—å —Å–ª–æ–µ–≤ –¥–ª—è transfer learning"""
        layers_to_freeze = int(len(self.network) * freeze_ratio)
        
        for i, layer in enumerate(self.network):
            if i < layers_to_freeze:
                for param in layer.parameters():
                    param.requires_grad = False
            else:
                for param in layer.parameters():
                    param.requires_grad = True
        
        logger.info(f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω–æ {layers_to_freeze} –∏–∑ {len(self.network)} —Å–ª–æ–µ–≤")
    
    def unfreeze_all_layers(self):
        """–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Å–ª–æ–∏"""
        for layer in self.network:
            for param in layer.parameters():
                param.requires_grad = True
        logger.info("–í—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã") 